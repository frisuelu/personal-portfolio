import{S as Io,i as So,s as Lo,k as n,q as o,a as p,l as r,m as l,r as s,h as t,c,V as pt,n as m,b as h,O as a,R as Sa}from"./index.012e952f.js";function Co(Ro){let E,ct,Ie,q,mt,Se,U,D,La,Le,Ce,He,u,ut,ne,dt,ft,re,wt,vt,ie,yt,bt,le,gt,kt,he,Et,_t,Oe,_,Tt,pe,Nt,Pt,Fe,j,ce,At,Ge,z,Rt,qe,v,xt,me,Mt,It,R,St,Lt,Ue,B,Ct,De,y,Ht,ue,Ot,Ft,de,Gt,qt,je,d,Ut,x,Dt,jt,fe,zt,Bt,we,Vt,Kt,ze,V,Qt,Be,K,M,Q,Ca,Ve,T,Zt,ve,Yt,Jt,Ke,Z,I,Y,Ha,Qe,w,ye,Wt,Xt,be,$t,ea,ge,ta,aa,Ze,J,S,W,Oa,Ye,k,ke,oa,sa,Ee,na,ra,Je,L,_e,ia,la,We,N,ha,Te,pa,ca,Xe,P,ma,Ne,ua,da,$e,b,fa,Pe,wa,va,Ae,ya,ba,et,X,Re,ga,tt,g,ka,C,Ea,_a,H,Ta,Na,at,$,Pa,ot,O,xo=`<code class="language-python"><span class="token comment"># Import the libraries</span>
<span class="token keyword">from</span> transformers <span class="token keyword">import</span> MarianMTModel<span class="token punctuation">,</span> MarianTokenizer
<span class="token keyword">import</span> torch

<span class="token comment"># Load a pretrained "English to Spanish" model</span>
tokenizer <span class="token operator">=</span> MarianTokenizer<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>
    <span class="token string">"Helsinki-NLP/opus-mt-en-es"</span>
<span class="token punctuation">)</span>

model <span class="token operator">=</span> MarianMTModel<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>
    <span class="token string">"Helsinki-NLP/opus-mt-en-es"</span>
<span class="token punctuation">)</span>

<span class="token comment"># Input a sentence</span>
<span class="token builtin">input</span> <span class="token operator">=</span> tokenizer<span class="token punctuation">(</span>
    <span class="token punctuation">(</span>
        <span class="token string">"Transformers are a really cool tool for multiple NLP tasks, "</span>
        <span class="token string">"but they can do so much more!!"</span>
    <span class="token punctuation">)</span><span class="token punctuation">,</span>
    return_tensors <span class="token operator">=</span> <span class="token string">'pt'</span><span class="token punctuation">,</span>
    padding <span class="token operator">=</span> <span class="token boolean">True</span>
<span class="token punctuation">)</span>

<span class="token comment"># Print the results</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>
    tokenizer<span class="token punctuation">.</span>batch_decode<span class="token punctuation">(</span>
        model<span class="token punctuation">.</span>generate<span class="token punctuation">(</span><span class="token operator">**</span><span class="token builtin">input</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
        skip_special_tokens<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
    <span class="token punctuation">)</span></code>`,st,F,Aa,xe,Ra,nt,ee,xa;return{c(){E=n("h1"),ct=o("TRANSFORMERS - multi-purpose AI models in disguise"),Ie=p(),q=n("h3"),mt=o("Novel applications of this powerful architecture set the bar for future AI advances"),Se=p(),U=n("p"),D=n("img"),Le=p(),Ce=n("hr"),He=p(),u=n("p"),ut=o("If you have dug deep into "),ne=n("em"),dt=o("machine learning"),ft=o(" algorithms, you will probably have heard of terms such as "),re=n("em"),wt=o("neural networks"),vt=o(" or "),ie=n("em"),yt=o("natural language processing"),bt=o(" (NLP). Regarding the latter, a powerful model architecture has appeared in the last few years that has disrupted the "),le=n("em"),gt=o("text mining"),kt=o(" industry: "),he=n("strong"),Et=o("The Transformer"),_t=o(". This model has altered the way researchers focus on analysing texts, introducing a novel analysis that has improved the models used previously. In the NLP field, it has become the game-changer mechanism and it is the main focus of research around the world. This has brought the model wide recognition, especially through developments such as OpenAI’s GPT-3 model for the generation of text."),Oe=p(),_=n("p"),Tt=o("Moreover, it has also been concluded that the architecture of Transformers is highly adaptable, hence applicable to tasks that may seem totally unrelated to each other. These applications could drive the development of new "),pe=n("em"),Nt=o("machine learning"),Pt=o(" algorithms that rely on this technology."),Fe=p(),j=n("blockquote"),ce=n("p"),At=o("The goal of this article is to present the Transformer in this new light, showing common applications and solutions that employ this model, but also remarking on the new and novel uses of this architecture that take into account its many advantages and high versatility."),Ge=p(),z=n("p"),Rt=o("So, a brief introduction to the Transformer, its beginnings, and the most common uses will be presented next. In the second part of this article, we will delve deeper into the new advances being made by the research community, presenting some exciting new use cases and code examples along the way. So strap in and prepare for the ride!"),qe=p(),v=n("p"),xt=o("It should be noted that AI solutions sometimes lack the responsibility and rigour required when practising "),me=n("em"),Mt=o("Data Science"),It=o(". The undesired effect is that models can retain the inherent bias of the data sets used to train them, and this can lead to fiascos such as "),R=n("a"),St=o("Google’s Photos app"),Lt=o("."),Ue=p(),B=n("h3"),Ct=o("TRANSFORMER: APPEARANCE & RESEARCH"),De=p(),y=n("p"),Ht=o("NLP is one of the cornerstones of Data Science, and it is involved in most of our daily routines: web search engines, online translations, or social networks are just some examples where AI algorithms are applied in the understanding of textual data. Until 2017, most research in this field was focused on developing better models based on "),ue=n("em"),Ot=o("recurrent"),Ft=o(" and "),de=n("em"),Gt=o("convolutional neural networks"),qt=o(". These models were the highest performers in terms of accuracy and explainability at the time, albeit at the cost of enormous processing power and long training times. This meant the focus of the whole research community was on how to make these models perform better, or how to reduce the machine processing costs. However, a bottleneck was quickly being reached in terms of computational power, and novel ways of analysing text were needed more than ever."),je=p(),d=n("p"),Ut=o("In December 2017, the Transformer model architecture was proposed by Google Brain and Google Research members in the "),x=n("a"),Dt=o("paper"),jt=p(),fe=n("em"),zt=o("Attention is all you need"),Bt=o(", providing a new approach to NLP tasks through "),we=n("em"),Vt=o("self-attention"),Kt=o(" technology. This architecture completely outperformed previous models, both in terms of accuracy and training time, and quickly became the state-of-the-art architecture for these applications."),ze=p(),V=n("p"),Qt=o("One question may come to your mind: How does a Transformer work? How and why is it better? Although we will avoid highly technical explanations, a basic grasp of the fundamentals for each model is needed to understand its many advantages."),Be=p(),K=n("p"),M=n("a"),Q=n("img"),Ve=p(),T=n("p"),Zt=o("Neural networks are connections of nodes that represent relationships between data. They consist of input nodes where data is introduced, intermediate layers where it is processed, and output nodes where the results are obtained. Each of these nodes performs an operation on the data (specifically a "),ve=n("em"),Yt=o("regression"),Jt=o(") that affects the final result."),Ke=p(),Z=n("p"),I=n("a"),Y=n("img"),Qe=p(),w=n("p"),ye=n("em"),Wt=o("Recurrent neural networks"),Xt=o(" or RNN also take into account the time dimension of the data, where the outcome is influenced by the previous value. This allows the previous state of the data to be kept and sent into the next value. A variation of the RNN named LSTM or "),be=n("em"),$t=o("long short-term memory"),ea=o(" also takes into account multiple points, so the result avoids "),ge=n("em"),ta=o("short-term memory issues"),aa=o(" with the model that the RNN usually presents."),Ze=p(),J=n("p"),S=n("a"),W=n("img"),Ye=p(),k=n("p"),ke=n("em"),oa=o("Convolutional neural networks"),sa=o(" or CNN apply a mathematical transformation called "),Ee=n("em"),na=o("convolution"),ra=o(" to the data over a sliding window; this essentially looks at small sections of the data to understand its overall structure, finding patterns or features. The architecture is especially useful for Computer Vision applications, where objects are detected after looking at pieces of each picture."),Je=p(),L=n("p"),_e=n("em"),ia=o("Recurrence"),la=o(" is the main advantage of these models and makes them particularly suited for Computer Vision applications, but it becomes a burden when dealing with text analysis and NLP. The computational power increase when dealing with more complex word relationships and context quickly became a limiting factor for the direct application of these models."),We=p(),N=n("p"),ha=o("The advantage of the Transformer is replacing "),Te=n("strong"),pa=o("recurrence for Attention"),ca=o(". Attention in this context is a relation mechanism that works “word-to-word”, computing the relationship of each word with the rest, including itself. Since this mathematically means products between word vector components, the computational cost needed is lower than recurrence methods."),Xe=p(),P=n("p"),ma=o("In the original Transformer architecture, this mechanism is actually "),Ne=n("em"),ua=o("multi-headed attention"),da=o(" that runs these operations in parallel to both speed the calculations, as well as to learn different interpretations for the same sentence. Although other factors are involved, this fact is the main reason why the Transformer takes less time to be trained and produces better results than its counterparts, and the reason why it is the predominant algorithm in NLP."),$e=p(),b=n("p"),fa=o("If you want to learn more about the original Transformer and its most famous variants, I suggest you take a look at "),Pe=n("em"),wa=o("Transformers for Natural Language Processing"),va=o(" by "),Ae=n("em"),ya=o("Denis Rothman"),ba=o("; it includes a hands-on explanation and coding lines for each step performed by the model, which helps to understand its inner workings."),et=p(),X=n("h3"),Re=n("strong"),ga=o("A SIMPLE AND QUICK USE"),tt=p(),g=n("p"),ka=o("Another great thing about the Transformer research community is the willingness to share and spread knowledge. The online community "),C=n("a"),Ea=o("HuggingFace"),_a=o(" provides a model repository, a Python library, and plenty of documentation to use and train new models based on the available frameworks developed by researchers. They also provide a "),H=n("a"),Ta=o("course"),Na=o(" for those interested in learning about their platform, so this should be the first stop for you, as an interested reader, if you aim to learn more about the current state-of-the-art models!"),at=p(),$=n("p"),Pa=o("Using these models is also very easy with the help of their library, in just a few lines of code we can use pre-trained models for different tasks. One of those is the use of over 1000 translation models developed by the University of Helsinki:"),ot=p(),O=n("pre"),st=p(),F=n("p"),Aa=o("The output is the sentence: "),xe=n("strong"),Ra=o("Los transformadores son una herramienta realmente genial para múltiples tareas NLP, pero pueden hacer mucho más!!"),nt=p(),ee=n("p"),xa=o("Stay tuned for the next part of this article, where we will present cutting-edge uses of the Transformer in other areas of application of AI, where previously other models reigned supreme."),this.h()},l(e){E=r(e,"H1",{});var i=l(E);ct=s(i,"TRANSFORMERS - multi-purpose AI models in disguise"),i.forEach(t),Ie=c(e),q=r(e,"H3",{});var Fa=l(q);mt=s(Fa,"Novel applications of this powerful architecture set the bar for future AI advances"),Fa.forEach(t),Se=c(e),U=r(e,"P",{});var Ga=l(U);D=r(Ga,"IMG",{src:!0,alt:!0}),Ga.forEach(t),Le=c(e),Ce=r(e,"HR",{}),He=c(e),u=r(e,"P",{});var f=l(u);ut=s(f,"If you have dug deep into "),ne=r(f,"EM",{});var qa=l(ne);dt=s(qa,"machine learning"),qa.forEach(t),ft=s(f," algorithms, you will probably have heard of terms such as "),re=r(f,"EM",{});var Ua=l(re);wt=s(Ua,"neural networks"),Ua.forEach(t),vt=s(f," or "),ie=r(f,"EM",{});var Da=l(ie);yt=s(Da,"natural language processing"),Da.forEach(t),bt=s(f," (NLP). Regarding the latter, a powerful model architecture has appeared in the last few years that has disrupted the "),le=r(f,"EM",{});var ja=l(le);gt=s(ja,"text mining"),ja.forEach(t),kt=s(f," industry: "),he=r(f,"STRONG",{});var za=l(he);Et=s(za,"The Transformer"),za.forEach(t),_t=s(f,". This model has altered the way researchers focus on analysing texts, introducing a novel analysis that has improved the models used previously. In the NLP field, it has become the game-changer mechanism and it is the main focus of research around the world. This has brought the model wide recognition, especially through developments such as OpenAI’s GPT-3 model for the generation of text."),f.forEach(t),Oe=c(e),_=r(e,"P",{});var rt=l(_);Tt=s(rt,"Moreover, it has also been concluded that the architecture of Transformers is highly adaptable, hence applicable to tasks that may seem totally unrelated to each other. These applications could drive the development of new "),pe=r(rt,"EM",{});var Ba=l(pe);Nt=s(Ba,"machine learning"),Ba.forEach(t),Pt=s(rt," algorithms that rely on this technology."),rt.forEach(t),Fe=c(e),j=r(e,"BLOCKQUOTE",{});var Va=l(j);ce=r(Va,"P",{});var Ka=l(ce);At=s(Ka,"The goal of this article is to present the Transformer in this new light, showing common applications and solutions that employ this model, but also remarking on the new and novel uses of this architecture that take into account its many advantages and high versatility."),Ka.forEach(t),Va.forEach(t),Ge=c(e),z=r(e,"P",{});var Qa=l(z);Rt=s(Qa,"So, a brief introduction to the Transformer, its beginnings, and the most common uses will be presented next. In the second part of this article, we will delve deeper into the new advances being made by the research community, presenting some exciting new use cases and code examples along the way. So strap in and prepare for the ride!"),Qa.forEach(t),qe=c(e),v=r(e,"P",{});var te=l(v);xt=s(te,"It should be noted that AI solutions sometimes lack the responsibility and rigour required when practising "),me=r(te,"EM",{});var Za=l(me);Mt=s(Za,"Data Science"),Za.forEach(t),It=s(te,". The undesired effect is that models can retain the inherent bias of the data sets used to train them, and this can lead to fiascos such as "),R=r(te,"A",{href:!0,rel:!0});var Ya=l(R);St=s(Ya,"Google’s Photos app"),Ya.forEach(t),Lt=s(te,"."),te.forEach(t),Ue=c(e),B=r(e,"H3",{});var Ja=l(B);Ct=s(Ja,"TRANSFORMER: APPEARANCE & RESEARCH"),Ja.forEach(t),De=c(e),y=r(e,"P",{});var ae=l(y);Ht=s(ae,"NLP is one of the cornerstones of Data Science, and it is involved in most of our daily routines: web search engines, online translations, or social networks are just some examples where AI algorithms are applied in the understanding of textual data. Until 2017, most research in this field was focused on developing better models based on "),ue=r(ae,"EM",{});var Wa=l(ue);Ot=s(Wa,"recurrent"),Wa.forEach(t),Ft=s(ae," and "),de=r(ae,"EM",{});var Xa=l(de);Gt=s(Xa,"convolutional neural networks"),Xa.forEach(t),qt=s(ae,". These models were the highest performers in terms of accuracy and explainability at the time, albeit at the cost of enormous processing power and long training times. This meant the focus of the whole research community was on how to make these models perform better, or how to reduce the machine processing costs. However, a bottleneck was quickly being reached in terms of computational power, and novel ways of analysing text were needed more than ever."),ae.forEach(t),je=c(e),d=r(e,"P",{});var A=l(d);Ut=s(A,"In December 2017, the Transformer model architecture was proposed by Google Brain and Google Research members in the "),x=r(A,"A",{href:!0,rel:!0});var $a=l(x);Dt=s($a,"paper"),$a.forEach(t),jt=c(A),fe=r(A,"EM",{});var eo=l(fe);zt=s(eo,"Attention is all you need"),eo.forEach(t),Bt=s(A,", providing a new approach to NLP tasks through "),we=r(A,"EM",{});var to=l(we);Vt=s(to,"self-attention"),to.forEach(t),Kt=s(A," technology. This architecture completely outperformed previous models, both in terms of accuracy and training time, and quickly became the state-of-the-art architecture for these applications."),A.forEach(t),ze=c(e),V=r(e,"P",{});var ao=l(V);Qt=s(ao,"One question may come to your mind: How does a Transformer work? How and why is it better? Although we will avoid highly technical explanations, a basic grasp of the fundamentals for each model is needed to understand its many advantages."),ao.forEach(t),Be=c(e),K=r(e,"P",{});var oo=l(K);M=r(oo,"A",{href:!0,rel:!0});var so=l(M);Q=r(so,"IMG",{src:!0,alt:!0}),so.forEach(t),oo.forEach(t),Ve=c(e),T=r(e,"P",{});var it=l(T);Zt=s(it,"Neural networks are connections of nodes that represent relationships between data. They consist of input nodes where data is introduced, intermediate layers where it is processed, and output nodes where the results are obtained. Each of these nodes performs an operation on the data (specifically a "),ve=r(it,"EM",{});var no=l(ve);Yt=s(no,"regression"),no.forEach(t),Jt=s(it,") that affects the final result."),it.forEach(t),Ke=c(e),Z=r(e,"P",{});var ro=l(Z);I=r(ro,"A",{href:!0,rel:!0});var io=l(I);Y=r(io,"IMG",{src:!0,alt:!0}),io.forEach(t),ro.forEach(t),Qe=c(e),w=r(e,"P",{});var G=l(w);ye=r(G,"EM",{});var lo=l(ye);Wt=s(lo,"Recurrent neural networks"),lo.forEach(t),Xt=s(G," or RNN also take into account the time dimension of the data, where the outcome is influenced by the previous value. This allows the previous state of the data to be kept and sent into the next value. A variation of the RNN named LSTM or "),be=r(G,"EM",{});var ho=l(be);$t=s(ho,"long short-term memory"),ho.forEach(t),ea=s(G," also takes into account multiple points, so the result avoids "),ge=r(G,"EM",{});var po=l(ge);ta=s(po,"short-term memory issues"),po.forEach(t),aa=s(G," with the model that the RNN usually presents."),G.forEach(t),Ze=c(e),J=r(e,"P",{});var co=l(J);S=r(co,"A",{href:!0,rel:!0});var mo=l(S);W=r(mo,"IMG",{src:!0,alt:!0}),mo.forEach(t),co.forEach(t),Ye=c(e),k=r(e,"P",{});var Me=l(k);ke=r(Me,"EM",{});var uo=l(ke);oa=s(uo,"Convolutional neural networks"),uo.forEach(t),sa=s(Me," or CNN apply a mathematical transformation called "),Ee=r(Me,"EM",{});var fo=l(Ee);na=s(fo,"convolution"),fo.forEach(t),ra=s(Me," to the data over a sliding window; this essentially looks at small sections of the data to understand its overall structure, finding patterns or features. The architecture is especially useful for Computer Vision applications, where objects are detected after looking at pieces of each picture."),Me.forEach(t),Je=c(e),L=r(e,"P",{});var Ma=l(L);_e=r(Ma,"EM",{});var wo=l(_e);ia=s(wo,"Recurrence"),wo.forEach(t),la=s(Ma," is the main advantage of these models and makes them particularly suited for Computer Vision applications, but it becomes a burden when dealing with text analysis and NLP. The computational power increase when dealing with more complex word relationships and context quickly became a limiting factor for the direct application of these models."),Ma.forEach(t),We=c(e),N=r(e,"P",{});var lt=l(N);ha=s(lt,"The advantage of the Transformer is replacing "),Te=r(lt,"STRONG",{});var vo=l(Te);pa=s(vo,"recurrence for Attention"),vo.forEach(t),ca=s(lt,". Attention in this context is a relation mechanism that works “word-to-word”, computing the relationship of each word with the rest, including itself. Since this mathematically means products between word vector components, the computational cost needed is lower than recurrence methods."),lt.forEach(t),Xe=c(e),P=r(e,"P",{});var ht=l(P);ma=s(ht,"In the original Transformer architecture, this mechanism is actually "),Ne=r(ht,"EM",{});var yo=l(Ne);ua=s(yo,"multi-headed attention"),yo.forEach(t),da=s(ht," that runs these operations in parallel to both speed the calculations, as well as to learn different interpretations for the same sentence. Although other factors are involved, this fact is the main reason why the Transformer takes less time to be trained and produces better results than its counterparts, and the reason why it is the predominant algorithm in NLP."),ht.forEach(t),$e=c(e),b=r(e,"P",{});var oe=l(b);fa=s(oe,"If you want to learn more about the original Transformer and its most famous variants, I suggest you take a look at "),Pe=r(oe,"EM",{});var bo=l(Pe);wa=s(bo,"Transformers for Natural Language Processing"),bo.forEach(t),va=s(oe," by "),Ae=r(oe,"EM",{});var go=l(Ae);ya=s(go,"Denis Rothman"),go.forEach(t),ba=s(oe,"; it includes a hands-on explanation and coding lines for each step performed by the model, which helps to understand its inner workings."),oe.forEach(t),et=c(e),X=r(e,"H3",{});var ko=l(X);Re=r(ko,"STRONG",{});var Eo=l(Re);ga=s(Eo,"A SIMPLE AND QUICK USE"),Eo.forEach(t),ko.forEach(t),tt=c(e),g=r(e,"P",{});var se=l(g);ka=s(se,"Another great thing about the Transformer research community is the willingness to share and spread knowledge. The online community "),C=r(se,"A",{href:!0,rel:!0});var _o=l(C);Ea=s(_o,"HuggingFace"),_o.forEach(t),_a=s(se," provides a model repository, a Python library, and plenty of documentation to use and train new models based on the available frameworks developed by researchers. They also provide a "),H=r(se,"A",{href:!0,rel:!0});var To=l(H);Ta=s(To,"course"),To.forEach(t),Na=s(se," for those interested in learning about their platform, so this should be the first stop for you, as an interested reader, if you aim to learn more about the current state-of-the-art models!"),se.forEach(t),at=c(e),$=r(e,"P",{});var No=l($);Pa=s(No,"Using these models is also very easy with the help of their library, in just a few lines of code we can use pre-trained models for different tasks. One of those is the use of over 1000 translation models developed by the University of Helsinki:"),No.forEach(t),ot=c(e),O=r(e,"PRE",{class:!0});var Mo=l(O);Mo.forEach(t),st=c(e),F=r(e,"P",{});var Ia=l(F);Aa=s(Ia,"The output is the sentence: "),xe=r(Ia,"STRONG",{});var Po=l(xe);Ra=s(Po,"Los transformadores son una herramienta realmente genial para múltiples tareas NLP, pero pueden hacer mucho más!!"),Po.forEach(t),Ia.forEach(t),nt=c(e),ee=r(e,"P",{});var Ao=l(ee);xa=s(Ao,"Stay tuned for the next part of this article, where we will present cutting-edge uses of the Transformer in other areas of application of AI, where previously other models reigned supreme."),Ao.forEach(t),this.h()},h(){pt(D.src,La="https://cdn-images-1.medium.com/max/1200/1*wZOvEaKZ5CUCgyFlvYfh3w.jpeg")||m(D,"src",La),m(D,"alt","Abstract wordcloud"),m(R,"href","https://www.bbc.com/news/technology-33347866"),m(R,"rel","nofollow"),m(x,"href","https://arxiv.org/abs/1706.03762"),m(x,"rel","nofollow"),pt(Q.src,Ca="https://cdn-images-1.medium.com/max/800/0*ybBwvf6QLBBQBHNE")||m(Q,"src",Ca),m(Q,"alt","Figure 1: schema of a neural network."),m(M,"href","https://www.w3schools.com/ai/ai_neural_networks.asp"),m(M,"rel","nofollow"),pt(Y.src,Ha="https://cdn-images-1.medium.com/max/800/0*VTQMRILKk_FbJt83")||m(Y,"src",Ha),m(Y,"alt","Figure 2: Graphical comparison between a neural network and a RNN. The _loop_ provides the time dimension to the model."),m(I,"href","https://www.researchgate.net/figure/The-comparison-between-Recurrent-Neural-Network-RNN-and-Feed-Forward-Neural-Network_fig1_338672883"),m(I,"rel","nofollow"),pt(W.src,Oa="https://cdn-images-1.medium.com/max/800/0*kz3juzVJCTezz-YP")||m(W,"src",Oa),m(W,"alt","Figure 3: schematic view of a CNN. Feature learning involves the training process, while classification is the model output."),m(S,"href","https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53"),m(S,"rel","nofollow"),m(C,"href","https://huggingface.co/"),m(C,"rel","nofollow"),m(H,"href","https://huggingface.co/course/"),m(H,"rel","nofollow"),m(O,"class","language-python")},m(e,i){h(e,E,i),a(E,ct),h(e,Ie,i),h(e,q,i),a(q,mt),h(e,Se,i),h(e,U,i),a(U,D),h(e,Le,i),h(e,Ce,i),h(e,He,i),h(e,u,i),a(u,ut),a(u,ne),a(ne,dt),a(u,ft),a(u,re),a(re,wt),a(u,vt),a(u,ie),a(ie,yt),a(u,bt),a(u,le),a(le,gt),a(u,kt),a(u,he),a(he,Et),a(u,_t),h(e,Oe,i),h(e,_,i),a(_,Tt),a(_,pe),a(pe,Nt),a(_,Pt),h(e,Fe,i),h(e,j,i),a(j,ce),a(ce,At),h(e,Ge,i),h(e,z,i),a(z,Rt),h(e,qe,i),h(e,v,i),a(v,xt),a(v,me),a(me,Mt),a(v,It),a(v,R),a(R,St),a(v,Lt),h(e,Ue,i),h(e,B,i),a(B,Ct),h(e,De,i),h(e,y,i),a(y,Ht),a(y,ue),a(ue,Ot),a(y,Ft),a(y,de),a(de,Gt),a(y,qt),h(e,je,i),h(e,d,i),a(d,Ut),a(d,x),a(x,Dt),a(d,jt),a(d,fe),a(fe,zt),a(d,Bt),a(d,we),a(we,Vt),a(d,Kt),h(e,ze,i),h(e,V,i),a(V,Qt),h(e,Be,i),h(e,K,i),a(K,M),a(M,Q),h(e,Ve,i),h(e,T,i),a(T,Zt),a(T,ve),a(ve,Yt),a(T,Jt),h(e,Ke,i),h(e,Z,i),a(Z,I),a(I,Y),h(e,Qe,i),h(e,w,i),a(w,ye),a(ye,Wt),a(w,Xt),a(w,be),a(be,$t),a(w,ea),a(w,ge),a(ge,ta),a(w,aa),h(e,Ze,i),h(e,J,i),a(J,S),a(S,W),h(e,Ye,i),h(e,k,i),a(k,ke),a(ke,oa),a(k,sa),a(k,Ee),a(Ee,na),a(k,ra),h(e,Je,i),h(e,L,i),a(L,_e),a(_e,ia),a(L,la),h(e,We,i),h(e,N,i),a(N,ha),a(N,Te),a(Te,pa),a(N,ca),h(e,Xe,i),h(e,P,i),a(P,ma),a(P,Ne),a(Ne,ua),a(P,da),h(e,$e,i),h(e,b,i),a(b,fa),a(b,Pe),a(Pe,wa),a(b,va),a(b,Ae),a(Ae,ya),a(b,ba),h(e,et,i),h(e,X,i),a(X,Re),a(Re,ga),h(e,tt,i),h(e,g,i),a(g,ka),a(g,C),a(C,Ea),a(g,_a),a(g,H),a(H,Ta),a(g,Na),h(e,at,i),h(e,$,i),a($,Pa),h(e,ot,i),h(e,O,i),O.innerHTML=xo,h(e,st,i),h(e,F,i),a(F,Aa),a(F,xe),a(xe,Ra),h(e,nt,i),h(e,ee,i),a(ee,xa)},p:Sa,i:Sa,o:Sa,d(e){e&&t(E),e&&t(Ie),e&&t(q),e&&t(Se),e&&t(U),e&&t(Le),e&&t(Ce),e&&t(He),e&&t(u),e&&t(Oe),e&&t(_),e&&t(Fe),e&&t(j),e&&t(Ge),e&&t(z),e&&t(qe),e&&t(v),e&&t(Ue),e&&t(B),e&&t(De),e&&t(y),e&&t(je),e&&t(d),e&&t(ze),e&&t(V),e&&t(Be),e&&t(K),e&&t(Ve),e&&t(T),e&&t(Ke),e&&t(Z),e&&t(Qe),e&&t(w),e&&t(Ze),e&&t(J),e&&t(Ye),e&&t(k),e&&t(Je),e&&t(L),e&&t(We),e&&t(N),e&&t(Xe),e&&t(P),e&&t($e),e&&t(b),e&&t(et),e&&t(X),e&&t(tt),e&&t(g),e&&t(at),e&&t($),e&&t(ot),e&&t(O),e&&t(st),e&&t(F),e&&t(nt),e&&t(ee)}}}const Oo={title:"TRANSFORMERS - multi-purpose AI models in disguise (Part I)",subtitle:"First part of the post series delving into the Transformer architecture and its applications outside of NLP.",date:"2022-01-09",thumbnail_url:"https://miro.medium.com/v2/format:webp/1*wZOvEaKZ5CUCgyFlvYfh3w.jpeg",thumbnail_alt:"Transformers outside NLP",media:"Posted on Medium"};class Fo extends Io{constructor(E){super(),So(this,E,null,Co,Lo,{})}}export{Fo as default,Oo as metadata};
