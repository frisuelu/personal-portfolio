<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width" />
    <link rel="icon" href="https://fav.farm/üßëüèª‚Äçüíª" />
    
		<link href="../_app/immutable/assets/0.803a2e3e.css" rel="stylesheet">
		<link href="../_app/immutable/assets/5.7135dbff.css" rel="stylesheet">
		<link rel="modulepreload" href="../_app/immutable/entry/start.d630c0b9.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/index.27678611.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/singletons.d9bd768f.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/index.5e935230.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/paths.29bc9e92.js">
		<link rel="modulepreload" href="../_app/immutable/entry/app.4addf335.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/preload-helper.a4192956.js">
		<link rel="modulepreload" href="../_app/immutable/nodes/0.13bbb513.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/github.a3ef01e6.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/mail.ce886a80.js">
		<link rel="modulepreload" href="../_app/immutable/nodes/5.e9a4fcca.js"><title>TRANSFORMERS - multi-purpose AI models in disguise (Part I)</title><!-- HEAD_svelte-1opz4md_START --><meta charset="UTF-8"><meta name="description" content="Medium: $TRANSFORMERS - multi-purpose AI models in disguise (Part I)"><meta name="keywords" content="Data Science, AI, Machine Learning, Technology"><meta name="author" content="Manuel Souto Juan"><meta property="og:type" content="article"><meta property="og:title" content="TRANSFORMERS - multi-purpose AI models in disguise (Part I)"><!-- HEAD_svelte-1opz4md_END -->
    <!-- Check dark or light mode -->
    <script type="module">
      const theme = localStorage.getItem("color-scheme");

      theme
        ? document.documentElement.setAttribute("color-scheme", theme)
        : localStorage.setItem("color-scheme", "dark");
    </script>
  </head>
  <body data-sveltekit-preload-data="hover">
    <div style="display: contents">


<div class="app svelte-pwfez8">
    <div class="Header-Footer svelte-pwfez8"><nav class="svelte-gy9bn7">
  <a href="/personal-portfolio/" class="title svelte-gy9bn7"><b>Personal Portfolio</b></a>
  
  <ul class="links svelte-gy9bn7"><li><a href="/personal-portfolio/skillset" class="svelte-gy9bn7">Skillset</a></li>
    <li><a href="/personal-portfolio/projects" class="svelte-gy9bn7">Projects</a></li>
    <li><a href="/personal-portfolio/blog" class="svelte-gy9bn7">Blog</a></li>
    <li><a href="/personal-portfolio/conferences" class="svelte-gy9bn7">Conferences</a></li>
    <li><a href="/personal-portfolio/about" class="svelte-gy9bn7">About</a></li></ul>
  
  <button aria-label="Toggle theme" class="svelte-stng2m"><div class="svelte-stng2m"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide-icon lucide lucide-moon "><path d="M12 3a6 6 0 0 0 9 9 9 9 0 1 1-9-9Z"></path></svg>
      <span>Dark</span></div>
</button>
</nav></div>
    <main class="svelte-pwfez8"><link href="https://github.com/PrismJS/prism-themes/themes/prism-dracula.css" rel="stylesheet">




<article class="svelte-1pctfqm"><div class="prose"><h1>TRANSFORMERS - multi-purpose AI models in disguise</h1>
<h3>Novel applications of this powerful architecture set the bar for future AI advances</h3>
<p><img src="https://cdn-images-1.medium.com/max/1200/1*wZOvEaKZ5CUCgyFlvYfh3w.jpeg" alt="Abstract wordcloud"></p>
<hr>
<p>If you have dug deep into <em>machine learning</em> algorithms, you will probably have heard of terms such as <em>neural networks</em> or <em>natural language processing</em> (NLP). Regarding the latter, a powerful model architecture has appeared in the last few years that has disrupted the <em>text mining</em> industry: <strong>The Transformer</strong>. This model has altered the way researchers focus on analysing texts, introducing a novel analysis that has improved the models used previously. In the NLP field, it has become the game-changer mechanism and it is the main focus of research around the world. This has brought the model wide recognition, especially through developments such as OpenAI‚Äôs GPT-3 model for the generation of text.</p>
<p>Moreover, it has also been concluded that the architecture of Transformers is highly adaptable, hence applicable to tasks that may seem totally unrelated to each other. These applications could drive the development of new <em>machine learning</em> algorithms that rely on this technology.</p>
<blockquote><p>The goal of this article is to present the Transformer in this new light, showing common applications and solutions that employ this model, but also remarking on the new and novel uses of this architecture that take into account its many advantages and high versatility.</p></blockquote>
<p>So, a brief introduction to the Transformer, its beginnings, and the most common uses will be presented next. In the second part of this article, we will delve deeper into the new advances being made by the research community, presenting some exciting new use cases and code examples along the way. So strap in and prepare for the ride!</p>
<p>It should be noted that AI solutions sometimes lack the responsibility and rigour required when practising <em>Data Science</em>. The undesired effect is that models can retain the inherent bias of the data sets used to train them, and this can lead to fiascos such as <a href="https://www.bbc.com/news/technology-33347866" rel="nofollow">Google‚Äôs Photos app</a>.</p>
<h3>TRANSFORMER: APPEARANCE &amp;¬†RESEARCH</h3>
<p>NLP is one of the cornerstones of Data Science, and it is involved in most of our daily routines: web search engines, online translations, or social networks are just some examples where AI algorithms are applied in the understanding of textual data. Until 2017, most research in this field was focused on developing better models based on <em>recurrent</em> and <em>convolutional neural networks</em>. These models were the highest performers in terms of accuracy and explainability at the time, albeit at the cost of enormous processing power and long training times. This meant the focus of the whole research community was on how to make these models perform better, or how to reduce the machine processing costs. However, a bottleneck was quickly being reached in terms of computational power, and novel ways of analysing text were needed more than ever.</p>
<p>In December 2017, the Transformer model architecture was proposed by Google Brain and Google Research members in the <a href="https://arxiv.org/abs/1706.03762" rel="nofollow">paper</a> <em>Attention is all you need</em>, providing a new approach to NLP tasks through <em>self-attention</em> technology. This architecture completely outperformed previous models, both in terms of accuracy and training time, and quickly became the state-of-the-art architecture for these applications.</p>
<p>One question may come to your mind: How does a Transformer work? How and why is it better? Although we will avoid highly technical explanations, a basic grasp of the fundamentals for each model is needed to understand its many advantages.</p>
<p><a href="https://www.w3schools.com/ai/ai_neural_networks.asp" rel="nofollow"><img src="https://cdn-images-1.medium.com/max/800/0*ybBwvf6QLBBQBHNE" alt="Figure 1: schema of a neural¬†network."></a></p>
<p>Neural networks are connections of nodes that represent relationships between data. They consist of input nodes where data is introduced, intermediate layers where it is processed, and output nodes where the results are obtained. Each of these nodes performs an operation on the data (specifically a <em>regression</em>) that affects the final result.</p>
<p><a href="https://www.researchgate.net/figure/The-comparison-between-Recurrent-Neural-Network-RNN-and-Feed-Forward-Neural-Network_fig1_338672883" rel="nofollow"><img src="https://cdn-images-1.medium.com/max/800/0*VTQMRILKk_FbJt83" alt="Figure 2: Graphical comparison between a neural network and a RNN. The _loop_ provides the time dimension to the¬†model."></a></p>
<p><em>Recurrent neural networks</em> or RNN also take into account the time dimension of the data, where the outcome is influenced by the previous value. This allows the previous state of the data to be kept and sent into the next value. A variation of the RNN named LSTM or <em>long short-term memory</em> also takes into account multiple points, so the result avoids <em>short-term memory issues</em> with the model that the RNN usually presents.</p>
<p><a href="https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53" rel="nofollow"><img src="https://cdn-images-1.medium.com/max/800/0*kz3juzVJCTezz-YP" alt="Figure 3: schematic view of a CNN. Feature learning involves the training process, while classification is the model¬†output."></a></p>
<p><em>Convolutional neural networks</em> or CNN apply a mathematical transformation called <em>convolution</em> to the data over a sliding window; this essentially looks at small sections of the data to understand its overall structure, finding patterns or features. The architecture is especially useful for Computer Vision applications, where objects are detected after looking at pieces of each picture.</p>
<p><em>Recurrence</em> is the main advantage of these models and makes them particularly suited for Computer Vision applications, but it becomes a burden when dealing with text analysis and NLP. The computational power increase when dealing with more complex word relationships and context quickly became a limiting factor for the direct application of these models.</p>
<p>The advantage of the Transformer is replacing <strong>recurrence for Attention</strong>. Attention in this context is a relation mechanism that works ‚Äúword-to-word‚Äù, computing the relationship of each word with the rest, including itself. Since this mathematically means products between word vector components, the computational cost needed is lower than recurrence methods.</p>
<p>In the original Transformer architecture, this mechanism is actually <em>multi-headed attention</em> that runs these operations in parallel to both speed the calculations, as well as to learn different interpretations for the same sentence. Although other factors are involved, this fact is the main reason why the Transformer takes less time to be trained and produces better results than its counterparts, and the reason why it is the predominant algorithm in NLP.</p>
<p>If you want to learn more about the original Transformer and its most famous variants, I suggest you take a look at <em>Transformers for Natural Language Processing</em> by <em>Denis Rothman</em>; it includes a hands-on explanation and coding lines for each step performed by the model, which helps to understand its inner workings.</p>
<h3><strong>A SIMPLE AND QUICK¬†USE</strong></h3>
<p>Another great thing about the Transformer research community is the willingness to share and spread knowledge. The online community <a href="https://huggingface.co/" rel="nofollow">HuggingFace</a> provides a model repository, a Python library, and plenty of documentation to use and train new models based on the available frameworks developed by researchers. They also provide a <a href="https://huggingface.co/course/" rel="nofollow">course</a> for those interested in learning about their platform, so this should be the first stop for you, as an interested reader, if you aim to learn more about the current state-of-the-art models!</p>
<p>Using these models is also very easy with the help of their library, in just a few lines of code we can use pre-trained models for different tasks. One of those is the use of over 1000 translation models developed by the University of Helsinki:</p>
<pre class="language-python"><!-- HTML_TAG_START --><code class="language-python"><span class="token comment"># Import the libraries</span>
<span class="token keyword">from</span> transformers <span class="token keyword">import</span> MarianMTModel<span class="token punctuation">,</span> MarianTokenizer
<span class="token keyword">import</span> torch

<span class="token comment"># Load a pretrained "English to Spanish" model</span>
tokenizer <span class="token operator">=</span> MarianTokenizer<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>
    <span class="token string">"Helsinki-NLP/opus-mt-en-es"</span>
<span class="token punctuation">)</span>

model <span class="token operator">=</span> MarianMTModel<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>
    <span class="token string">"Helsinki-NLP/opus-mt-en-es"</span>
<span class="token punctuation">)</span>

<span class="token comment"># Input a sentence</span>
<span class="token builtin">input</span> <span class="token operator">=</span> tokenizer<span class="token punctuation">(</span>
    <span class="token punctuation">(</span>
        <span class="token string">"Transformers are a really cool tool for multiple NLP tasks, "</span>
        <span class="token string">"but they can do so much more!!"</span>
    <span class="token punctuation">)</span><span class="token punctuation">,</span>
    return_tensors <span class="token operator">=</span> <span class="token string">'pt'</span><span class="token punctuation">,</span>
    padding <span class="token operator">=</span> <span class="token boolean">True</span>
<span class="token punctuation">)</span>

<span class="token comment"># Print the results</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>
    tokenizer<span class="token punctuation">.</span>batch_decode<span class="token punctuation">(</span>
        model<span class="token punctuation">.</span>generate<span class="token punctuation">(</span><span class="token operator">**</span><span class="token builtin">input</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
        skip_special_tokens<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
    <span class="token punctuation">)</span></code><!-- HTML_TAG_END --></pre>
<p>The output is the sentence: <strong>Los transformadores son una herramienta realmente genial para m√∫ltiples tareas NLP, pero pueden hacer mucho m√°s!!</strong></p>
<p>Stay tuned for the next part of this article, where we will present cutting-edge uses of the Transformer in other areas of application of AI, where previously other models reigned supreme.</p></div>
</article></main>
    <div class="Header-Footer svelte-pwfez8"><footer class="svelte-i1r9nm"><nav class="svelte-i1r9nm">
    <a href="/personal-portfolio/" class="title svelte-i1r9nm"><b>Personal Portfolio</b></a>
    
    <ul class="links svelte-i1r9nm"><li><a href="/personal-portfolio/skillset" class="svelte-i1r9nm">Skillset</a></li>
      <li><a href="/personal-portfolio/projects" class="svelte-i1r9nm">Projects</a></li>
      <li><a href="/personal-portfolio/blog" class="svelte-i1r9nm">Blog</a></li>
      <li><a href="/personal-portfolio/conferences" class="svelte-i1r9nm">Conferences</a></li>
      <li><a href="/personal-portfolio/about" class="svelte-i1r9nm">About</a></li></ul>
    <div class="icons svelte-i1r9nm"><a href="https://www.linkedin.com/in/manuel-souto-juan/" class="svelte-i1r9nm"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide-icon lucide lucide-linkedin "><path d="M16 8a6 6 0 0 1 6 6v7h-4v-7a2 2 0 0 0-2-2 2 2 0 0 0-2 2v7h-4v-7a6 6 0 0 1 6-6z"></path><rect width="4" height="12" x="2" y="9"></rect><circle cx="4" cy="4" r="2"></circle></svg></a>
      <a href="https://www.github.com/frisuelu" class="svelte-i1r9nm"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide-icon lucide lucide-github "><path d="M15 22v-4a4.8 4.8 0 0 0-1-3.5c3 0 6-2 6-5.5.08-1.25-.27-2.48-1-3.5.28-1.15.28-2.35 0-3.5 0 0-1 0-3 1.5-2.64-.5-5.36-.5-8 0C6 2 5 2 5 2c-.3 1.15-.3 2.35 0 3.5A5.403 5.403 0 0 0 4 9c0 3.5 3 5.5 6 5.5-.39.49-.68 1.05-.85 1.65-.17.6-.22 1.23-.15 1.85v4"></path><path d="M9 18c-4.51 2-5-2-7-2"></path></svg></a>
      <a href="mailto:manuelsoju@hotmail.com" class="svelte-i1r9nm"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide-icon lucide lucide-mail "><rect width="20" height="16" x="2" y="4" rx="2"></rect><path d="m22 7-8.97 5.7a1.94 1.94 0 0 1-2.06 0L2 7"></path></svg></a></div></nav>
  <p><small>Website created with Svelte and SvelteKit.</small></p>
</footer></div>
</div>


			
			<script>
				{
					__sveltekit_xzkhb1 = {
						assets: "/personal-portfolio",
						base: new URL("..", location).pathname.slice(0, -1),
						env: {}
					};

					const element = document.currentScript.parentElement;

					const data = [null,null];

					Promise.all([
						import("../_app/immutable/entry/start.d630c0b9.js"),
						import("../_app/immutable/entry/app.4addf335.js")
					]).then(([kit, app]) => {
						kit.start(app, element, {
							node_ids: [0, 5],
							data,
							form: null,
							error: null
						});
					});
				}
			</script>
		</div>
  </body>
</html>
